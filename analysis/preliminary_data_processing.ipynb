{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_p = pd.read_csv('./data/positive_ten.csv',index_col=0)\n",
    "ten_n = pd.read_csv('./data/negative_ten.csv',index_col=0)\n",
    "n_tst = pd.read_csv('./data/negative_test.csv',index_col=0)\n",
    "p_tst = pd.read_csv('./data/positive_test.csv',index_col=0)\n",
    "n_tr = pd.read_csv('./data/negative_train.csv',index_col=0)\n",
    "p_tr = pd.read_csv('./data/positive_train.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['statuses_count', 'followers_count', 'friends_count', 'listed_count',\n",
       "       'favourites_count', 'favorite_count', 'retweet_count', 'days',\n",
       "       'posts_number_all', 'posts_number', 'text_length', 'reply_ratio',\n",
       "       'time', 'positive_words_count', 'negative_words_count',\n",
       "       'positive_emoji_count', 'negative_emoji_count', 'vad',\n",
       "       'antidepressant_words_count', 'depression_symptoms_count', 'lda'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_ten = [ten_p,ten_n]\n",
    "frames_full = [n_tst,n_tr,p_tst,p_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_concat = pd.concat(frames_ten)\n",
    "whole_concat = pd.concat(frames_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6348"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(whole_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6348"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ten_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(whole_concat.index.tolist())==set(ten_concat.index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove biased features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compared two datasets - if there are same feature values, it is most probably biased - have to be removed\n",
    "#in oder to be 100%, I validate if the set of features is consistent for 1k samples\n",
    "\n",
    "examp_id = whole_concat.iloc[102].name\n",
    "c1 = whole_concat.loc[examp_id]\n",
    "c2 = ten_concat.loc[examp_id]\n",
    "biased_feats = []\n",
    "for i in c1.index:\n",
    "    if c1[i]==c2[i]:\n",
    "        #print(i)\n",
    "        biased_feats.append(i)\n",
    "\n",
    "\n",
    "for ii in range(1000):\n",
    "    examp_id = whole_concat.iloc[102].name\n",
    "    c1 = whole_concat.loc[examp_id]\n",
    "    c2 = ten_concat.loc[examp_id]\n",
    "    bb = []\n",
    "    for i in c1.index:\n",
    "        if c1[i]==c2[i]:\n",
    "            bb.append(i)\n",
    "    if str(bb) != str(biased_feats):\n",
    "        print('there is a problem with biased features: ',bb)\n",
    "        print('it does not agree with: ',biased_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ten = ten_concat.drop(labels=biased_feats,axis=1)\n",
    "clean_whole = whole_concat.drop(labels=biased_feats,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding features, processing lists, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['favorite_count', 'retweet_count', 'days', 'posts_number_all',\n",
       "       'text_length', 'reply_ratio', 'time', 'positive_words_count',\n",
       "       'negative_words_count', 'positive_emoji_count', 'vad',\n",
       "       'depression_symptoms_count', 'lda'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ten.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vad - always 3 values\n",
    "#### time - always 24\n",
    "#### depression_symptoms_count - always 9\n",
    "#### LDA - always 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(clean_ten.time.iloc[101][1:-1].split(',')).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(clean_ten.lda.iloc[111][1:-1].split(',')).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(clean_ten.depression_symptoms_count.iloc[101][1:-1].split(',')).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(clean_ten.vad.iloc[101][1:-1].split(',')).astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features stationarization\n",
    "#### retweet_count should be considered with respect to the posts_number_all or text_length(?)\n",
    "#### text_length - with respect to the posts_number_all\n",
    "#### reply_ratio - with respect to the retweet_count (assuming there is a correlation of retweet_count and number of comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ten['retweet_count_div_by_posts_number_all'] = clean_ten.retweet_count/clean_ten.posts_number_all\n",
    "clean_whole['retweet_count_div_by_posts_number_all'] = clean_whole.retweet_count/clean_whole.posts_number_all\n",
    "\n",
    "clean_ten['positive_words_freq'] = clean_ten.positive_words_count/(clean_ten.posts_number_all*clean_ten.text_length)\n",
    "clean_whole['positive_words_freq'] = clean_whole.positive_words_count/(clean_whole.posts_number_all*clean_whole.text_length)\n",
    "\n",
    "clean_ten['negavite_words_freq'] = clean_ten.negative_words_count/(clean_ten.posts_number_all*clean_ten.text_length)\n",
    "clean_whole['negative_words_freq'] = clean_whole.negative_words_count/(clean_whole.posts_number_all*clean_whole.text_length)\n",
    "\n",
    "clean_ten['positive_emoji_freq'] = clean_ten.positive_emoji_count/(clean_ten.posts_number_all*clean_ten.text_length)\n",
    "clean_whole['positive_emoji_freq'] = clean_whole.positive_emoji_count/(clean_whole.posts_number_all*clean_whole.text_length)\n",
    "\n",
    "#clean_ten['reply_ratio_div_by_retweet_count_plus_one'] = clean_ten.reply_ratio/(clean_ten.retweet_count+1.0)\n",
    "#clean_whole['reply_ratio_div_by_retweet_count_plus_one'] = clean_whole.reply_ratio/(clean_whole.retweet_count+1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonstat_to_remove = ['retweet_count','positive_words_count','negative_words_count','positive_emoji_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-stationary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ten = clean_ten.drop(labels=nonstat_to_remove,axis=1)\n",
    "clean_whole = clean_whole.drop(labels=nonstat_to_remove,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform lists into separate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ten.to_csv('./data/intermediate_ten.csv')\n",
    "clean_whole.to_csv('./data/intermediate_whole.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(x):\n",
    "    return np.array(literal_eval(x),dtype=np.float64).squeeze()\n",
    "\n",
    "converters={'vad': converter,'time': converter,'depression_symptoms_count': converter,'lda': converter}\n",
    "\n",
    "clean_ten = pd.read_csv('./data/intermediate_ten.csv',sep=',', converters=converters,index_col=0)\n",
    "clean_whole = pd.read_csv('./data/intermediate_whole.csv',sep=',', converters=converters,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating names for additional columns in df's\n",
    "vad_ls = []\n",
    "for i in range(3):\n",
    "    vad_ls.append('vad_'+str(i))\n",
    "    \n",
    "time_ls = []\n",
    "for i in range(24):\n",
    "    time_ls.append('time_'+str(i))\n",
    "    \n",
    "depression_symptoms_count_ls = []\n",
    "for i in range(9):\n",
    "    depression_symptoms_count_ls.append('depression_symptoms_count_'+str(i))\n",
    "    \n",
    "lda_ls = []\n",
    "for i in range(25):\n",
    "    lda_ls.append('lda_'+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all the features into final df's and save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_df_10 = pd.DataFrame(np.array(clean_ten.vad.values.tolist(),dtype=np.float64),index = clean_ten.index.tolist(),columns = vad_ls)\n",
    "time_df_10 = pd.DataFrame(np.array(clean_ten.time.values.tolist(),dtype=np.float64),index = clean_ten.index.tolist(),columns = time_ls)\n",
    "depression_symptoms_count_df_10 = pd.DataFrame(np.array(clean_ten.depression_symptoms_count.values.tolist(),dtype=np.float64),index = clean_ten.index.tolist(),columns = depression_symptoms_count_ls)\n",
    "lda_df_10 = pd.DataFrame(np.array(clean_ten.lda.values.tolist(),dtype=np.float64),index = clean_ten.index.tolist(),columns = lda_ls)\n",
    "FINAL_df_10 = pd.concat([clean_ten.drop(labels=['vad','lda','depression_symptoms_count','time'],axis=1),vad_df_10,time_df_10,depression_symptoms_count_df_10,lda_df_10],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_df_whole = pd.DataFrame(np.array(clean_whole.vad.values.tolist(),dtype=np.float64),index = clean_whole.index.tolist(),columns = vad_ls)\n",
    "time_df_whole = pd.DataFrame(np.array(clean_whole.time.values.tolist(),dtype=np.float64),index = clean_whole.index.tolist(),columns = time_ls)\n",
    "depression_symptoms_count_df_whole = pd.DataFrame(np.array(clean_whole.depression_symptoms_count.values.tolist(),dtype=np.float64),index = clean_whole.index.tolist(),columns = depression_symptoms_count_ls)\n",
    "lda_df_whole = pd.DataFrame(np.array(clean_whole.lda.values.tolist(),dtype=np.float64),index = clean_whole.index.tolist(),columns = lda_ls)\n",
    "FINAL_df_whole = pd.concat([clean_whole.drop(labels=['vad','lda','depression_symptoms_count','time'],axis=1),vad_df_whole,time_df_whole,depression_symptoms_count_df_whole,lda_df_whole],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_df_10.to_csv('./data/final_10.csv')\n",
    "FINAL_df_whole.to_csv('./data/final_whole.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
